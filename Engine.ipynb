{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring the limits of Mix-training for NMT task on low resource languages.\n",
        "\n",
        "Authors:  \n",
        "- [Anam ur Rehman](inshaa307@gmail.com)\n",
        "- [Mohammed El Dor](mohammed.eldor@studenti.polito.it)\n",
        "- [Mohamad Mostafa](mohamad.mostafa@studenti.polito.it)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-SrddAN6v0J"
      },
      "source": [
        "# 1. Preliminaries\n",
        "This section is common among all upcoming training stages. \n",
        "This section will provide basis for next training phases.  \n",
        "Here, we import common set of libraries, Initialize the tokenizer and introduce the utility scripts used while finetuning the models.  \n",
        "The tokenizer is shared among all stages of fine tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrYKgAkDPk0Y"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-12T16:41:11.313523Z",
          "iopub.status.busy": "2022-02-12T16:41:11.31322Z",
          "iopub.status.idle": "2022-02-12T16:41:48.54183Z",
          "shell.execute_reply": "2022-02-12T16:41:48.540934Z",
          "shell.execute_reply.started": "2022-02-12T16:41:11.313489Z"
        },
        "id": "6SIOVZ-ifM5k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "\n",
        "# ! pip install transformers==4.12.5\n",
        "# ! pip install sentencepiece==0.1.96\n",
        "# ! pip install sacrebleu==2.0.0\n",
        "# ! pip install datasets==1.16.1\n",
        "\n",
        "# ! apt-get install sudo -y\n",
        "# ! sudo apt-get install git-lfs\n",
        "# ! git lfs install\n",
        "# ! git clone \"https://github.com/DeskDown/NMT.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T15:55:14.963636Z",
          "iopub.status.busy": "2022-02-10T15:55:14.963355Z",
          "iopub.status.idle": "2022-02-10T15:55:22.813071Z",
          "shell.execute_reply": "2022-02-10T15:55:22.812309Z",
          "shell.execute_reply.started": "2022-02-10T15:55:14.963599Z"
        },
        "id": "O7TzE2s9fVGl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset, load_dataset, load_metric\n",
        "from transformers import (\n",
        "    MarianTokenizer,\n",
        "    MBart50TokenizerFast,\n",
        "    AutoModelForSeq2SeqLM, \n",
        "    DataCollatorForSeq2Seq, \n",
        "    Seq2SeqTrainingArguments, \n",
        "    Seq2SeqTrainer,\n",
        "    utils\n",
        "                    )\n",
        "\n",
        "utils.logging.set_verbosity(50)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGMSwtcmPIis"
      },
      "outputs": [],
      "source": [
        "SEED = 99\n",
        "TOKEN = \"<ENTER YOUR HF TOKEN HERE>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvOeafu2FjO"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "Requirements:\n",
        "1. Provide the name for helper language ('ja' or 'zh') in the next section of code.  \n",
        "\n",
        "Method:\n",
        "1. A pretrained tokenizer is loaded from HF model hub based on the selected helper language.\n",
        "2. Tokens from ALT dataset languages are added in the vocabulary.\n",
        "3. Tokens were generated using utility script `./make_tokens.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T15:55:22.814747Z",
          "iopub.status.busy": "2022-02-10T15:55:22.814495Z",
          "iopub.status.idle": "2022-02-10T15:55:35.824012Z",
          "shell.execute_reply": "2022-02-10T15:55:35.823223Z",
          "shell.execute_reply.started": "2022-02-10T15:55:22.814711Z"
        },
        "id": "JAaKtSZu2FjP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Specify the language(s) to include in dataset\n",
        "helper = 'zh'\n",
        "\n",
        "model_name = {\n",
        "        \"zh\":\"Helsinki-NLP/opus-mt-en-zh\",\n",
        "        \"ja\":\"Helsinki-NLP/opus-tatoeba-en-ja\"\n",
        "    }\n",
        "\n",
        "utils.logging.set_verbosity(50)\n",
        "# languages_subset = ['hi']\n",
        "# all_ds = 'DeskDown/ALTDataset'\n",
        "# alt_ds = load_dataset(all_ds)\n",
        "# sub_ds = alt_ds.filter(lambda example: example['lang_yy'] in languages_subset)\n",
        "\n",
        "\n",
        "def init_tokenizer(model_name):\n",
        "    utils.logging.set_verbosity(50)\n",
        "    # Load the fresh tokenizer from HF hub\n",
        "    marian_model_name = model_name\n",
        "    marian_tokenizer = MarianTokenizer.from_pretrained(marian_model_name)\n",
        "\n",
        "    # Add tokens to enable tokenization of languages not originaly supported by Marian tokenizer\n",
        "    tokens_file = \"NMT/tokens.txt\"\n",
        "    assert os.path.isfile(tokens_file), \"Clone the github repository: \\\n",
        "        git clone https://github.com/DeskDown/NMT.git\"\n",
        "\n",
        "    with open(tokens_file) as fp:\n",
        "        tokens = fp.read().split(\"\\n\")\n",
        "\n",
        "    marian_tokenizer.add_tokens(tokens, special_tokens=True)\n",
        "    return marian_tokenizer\n",
        "\n",
        "marian_tokenizer = init_tokenizer(model_name[helper])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAaNIqZxhY7u"
      },
      "source": [
        "## Preprocessing and compute metrics\n",
        "\n",
        "Requirements:\n",
        "Provide values for these parameters in next section  \n",
        "1. max_input_length\n",
        "2. max_output_length\n",
        "3. batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T15:55:35.826675Z",
          "iopub.status.busy": "2022-02-10T15:55:35.826288Z",
          "iopub.status.idle": "2022-02-10T15:55:36.592081Z",
          "shell.execute_reply": "2022-02-10T15:55:36.591482Z",
          "shell.execute_reply.started": "2022-02-10T15:55:35.826636Z"
        },
        "id": "J2rfMH4PNiCb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "batch_size = 64\n",
        "\n",
        "columns_to_transform = ['input_ids', 'labels', 'attention_mask']\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "marian_model_name = model_name[helper]\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    utils.logging.set_verbosity(50)\n",
        "    tokenizer = marian_tokenizer\n",
        "    \n",
        "    inputs = [s for s in examples[\"Sent_en\"]]\n",
        "    targets = [s for s in examples[\"Sent_yy\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    if len(examples[\"Sent_en\"]) > 1 and (len(model_inputs[\"input_ids\"][0])!=len(model_inputs[\"input_ids\"][1])):\n",
        "        print (\"Error!\", )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds, tokenizer = None):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = marian_tokenizer\n",
        "    \n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "        \n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds =  [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels] \n",
        "    \n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    \n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQtctYq2FjQ"
      },
      "source": [
        "# 2. Pure-Finetuning the Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqvf8msgSESB"
      },
      "source": [
        "In this section we load the pre-trained model from huggingface hub and perform pure-finetuning using one single language subset from ALT dataset. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "Requirements:\n",
        "1. Provide `language_code` of ALT dataset language from set {'fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi'}\n",
        "\n",
        "Method:\n",
        "1. We load the custom ALT dataset from HF hub `DeskDown/ALTDataset` generated using utility script `make_dataset.py` and later uploaded to HF hub\n",
        "2. The desired language is filtered from the ALT dataset\n",
        "3. Encodings are generated using Marian tokenizer and we update the encoding matrix size for tranlation model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz8rKvjqTzP9"
      },
      "outputs": [],
      "source": [
        "all_ds = 'DeskDown/ALTDataset'\n",
        "alt_ds = load_dataset(all_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfP40X9B2FjR",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "language_code = 'hi'\n",
        "\n",
        "sub_ds = alt_ds.filter(lambda example: example['lang_yy'] == language_code)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(marian_model_name)\n",
        "model.resize_token_embeddings(len(marian_tokenizer))\n",
        "sub_ds = sub_ds.map(preprocess_function, batched=True, batch_size=batch_size*3)\n",
        "sub_ds.set_format(type='torch', columns=columns_to_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_bGxrCUSDT"
      },
      "source": [
        "## Training\n",
        "\n",
        "Requirements:  \n",
        "1. Provide Hyper parameters for finetuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV805a1ryTwd",
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(marian_tokenizer, model=model)\n",
        "source_lang = f\"en-{helper}\"\n",
        "target_lang = language_code\n",
        "model_name = f\"marianPFT_{source_lang}-{target_lang}\"\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    # Misc\n",
        "    output_dir = model_name,\n",
        "    seed = 99,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    log_level = 'warning',\n",
        "    disable_tqdm = False,\n",
        "    \n",
        "    # Hyper parameters\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps = 10,\n",
        "    num_train_epochs=15,\n",
        "    predict_with_generate=True,\n",
        "    remove_unused_columns = True,\n",
        "    fp16=True,\n",
        "\n",
        "    # Model backup\n",
        "    save_total_limit=3,\n",
        "    save_strategy = \"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=sub_ds[\"train\"],\n",
        "    eval_dataset=sub_ds[\"eval\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=marian_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNDXts4Zyaji",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWxg0RSG2FjS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.predict(sub_ds[\"test \"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtUgvzhf2FjS"
      },
      "source": [
        "# 3. Mix-FinueTuning\n",
        "\n",
        "In this section we load the pre-trained model from huggingface hub and perform Mix-finetuning using a subset of languages from ALT dataset and Helper language dataset which was used during pre-training of HF model.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Requirements:\n",
        "1. Provide `yy_mix_subset` list of ALT dataset languages from set {'fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi'}  \n",
        "    This subset is alongside helper dataset to further train the model.\n",
        "2. Provide `helper_size` which is the portion of helper corpus used for training. We used the minimum number of senetences of ALT dataset as helper_size.\n",
        "\n",
        "\n",
        "Method:\n",
        "1. We load the custom ALT dataset from HF hub `DeskDown/ALTDataset` generated using utility script `make_dataset.py` and later uploaded to HF hub\n",
        "2. The desired languages are filtered from the ALT dataset\n",
        "3. We load the helper dataset from HF hub and concetinate these datasets.\n",
        "4. Encodings are generated using Marian tokenizer and we update the encoding matrix size for tranlation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOGMT_cP2FjS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "yy_mix_subset = ['fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi']\n",
        "helper_size = 18_000\n",
        "\n",
        "\n",
        "def make_mix_training_dataset(\n",
        "    ALT_ds,\n",
        "    helper,\n",
        "    helper_size = 20_000,\n",
        "    yy_mix_subset = [\"vi\"]\n",
        "    ):\n",
        "    \n",
        "    pad = \"*\"*4\n",
        "    select_subset = lambda x: x[\"lang_yy\"] in yy_mix_subset\n",
        "\n",
        "    def pre_process_helper(example):\n",
        "        example[\"Sent_en\"] = example[\"translation\"][\"en\"]\n",
        "        example[\"Sent_yy\"] = example[\"translation\"][helper]\n",
        "\n",
        "        return example\n",
        "\n",
        "    \n",
        "    helper_ds = load_dataset(\"opus100\", f'en-{helper}', split = \"train\")\n",
        "    alt_train_ds = ALT_ds[\"train\"].filter(select_subset).remove_columns([\"SID\", \"lang_yy\"])\n",
        "    helper_train_ds = helper_ds.select(range(helper_size)).map(pre_process_helper).remove_columns(\"translation\")\n",
        "\n",
        "    print(f\"{pad}Alt dataset used for mix training{pad}\\nTotal Languages: {len(yy_mix_subset)}\\nSentences: {len(alt_train_ds)}\")\n",
        "    print(f\"{pad}Opus dataset used for mix training{pad}\\nLanguages: en-zh\\nSentences: {helper_train_ds}\")\n",
        "\n",
        "    mix_ds = concatenate_datasets([alt_train_ds, helper_train_ds])\n",
        "    print(f\"{pad}Final dataset for Mix Training{pad}\\nLanguages: {1+len(yy_mix_subset)}\\nSentences: {len(mix_ds)}\")\n",
        "    \n",
        "    return mix_ds.shuffle(seed = SEED)\n",
        "\n",
        "\n",
        "mix_ds = make_mix_training_dataset(alt_ds, helper, helper_size = helper_size, yy_mix_subset = yy_mix_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjayJQD72FjT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "mix_ds = mix_ds.map(preprocess_function, batched=True, batch_size=batch_size*3)\n",
        "mix_ds.set_format(type='torch', columns=columns_to_transform)\n",
        "\n",
        "mix_eval_ds = alt_ds[\"eval\"].filter(lambda x: x[\"lang_yy\"] in yy_mix_subset)\n",
        "mix_eval_ds = mix_eval_ds.map(preprocess_function, batched=True, batch_size=batch_size)\n",
        "mix_eval_ds.set_format(type='torch', columns=columns_to_transform)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name[helper])\n",
        "model.resize_token_embeddings(len(marian_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3QGm8iD2FjT"
      },
      "source": [
        "## Training\n",
        "\n",
        "Requirements:  \n",
        "1. Provide Hyper parameters for finetuning process.\n",
        "2. If you want to push the model of HF hub, Make sure you have provided the correct `TOKEN` code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8wCsJGp2FjT",
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "data_collator = DataCollatorForSeq2Seq(marian_tokenizer, model=model)\n",
        "model_name = f\"MarianMix_en-{helper}-10\"\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    # Misc\n",
        "    output_dir = model_name,\n",
        "    seed = 99,\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 10_000,\n",
        "    log_level = 'warning',\n",
        "    disable_tqdm = False,\n",
        "    \n",
        "    # Hyper parameters\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps = 10,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    remove_unused_columns = True,\n",
        "    fp16=True,\n",
        "\n",
        "    # Model backup\n",
        "    save_total_limit=3,\n",
        "    save_strategy = \"steps\", #\"epoch\",\n",
        "    save_steps = 10_000,\n",
        "    push_to_hub = True,\n",
        "    hub_token = TOKEN,\n",
        "    hub_model_id = model_name,\n",
        "    hub_strategy = \"checkpoint\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=mix_ds,\n",
        "    eval_dataset=mix_eval_ds,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=marian_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2DBVMoJ2FjT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxcAt8652FjT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(repo_path_or_name = model_name,\n",
        "                  use_auth_token = TOKEN, \n",
        "                  commit_message = f\"Mix Trained marian model over 9 low resource languages of ALT dataset + 18K senetences from en-{helper} opus dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrKIK8592FjU"
      },
      "source": [
        "# 4. Pure-Finetuning the Mix-trained model\n",
        "\n",
        "In this section we load the Mix-trained model from our huggingface hub and perform Pure-finetuning using one language from ALT dataset.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Requirements:\n",
        "1. Provide `language_code` of ALT dataset language from set {'fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi'}  \n",
        "\n",
        "Method:\n",
        "1. We load the custom ALT dataset from HF hub `DeskDown/ALTDataset` generated using utility script `make_dataset.py` and later uploaded to HF hub\n",
        "2. The desired language is filtered from the ALT dataset\n",
        "3. Encodings are generated using Marian tokenizer and we update the encoding matrix size for tranlation model\n",
        "4. We use the Mix-trained model `DeskDown/MarianMix_en-[ja|zh]-10` to be further fine-tuned on selected language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T15:55:40.788666Z",
          "iopub.status.busy": "2022-02-10T15:55:40.788373Z",
          "iopub.status.idle": "2022-02-10T15:56:10.944569Z",
          "shell.execute_reply": "2022-02-10T15:56:10.94374Z",
          "shell.execute_reply.started": "2022-02-10T15:55:40.788634Z"
        },
        "id": "U1v1W6Dd2FjU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def init_mix_model(size, name):\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
        "    model.resize_token_embeddings(size)\n",
        "    return model\n",
        "\n",
        "alt_ds = load_dataset('DeskDown/ALTDataset')\n",
        "tokenizer = init_tokenizer(model_name[helper])\n",
        "model = init_mix_model(len(tokenizer), name = \"DeskDown/MarianMix_en-ja-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T17:10:51.490708Z",
          "iopub.status.busy": "2022-02-10T17:10:51.490155Z",
          "iopub.status.idle": "2022-02-10T17:12:36.885485Z",
          "shell.execute_reply": "2022-02-10T17:12:36.884646Z",
          "shell.execute_reply.started": "2022-02-10T17:10:51.490669Z"
        },
        "id": "mRqT8iW22FjU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "language_code = 'th'\n",
        "\n",
        "\n",
        "mix_trainded_model = \"DeskDown/MarianMix_en-ja-10\"\n",
        "fil = lambda x: x[\"lang_yy\"] == language_code\n",
        "model = init_mix_model(len(tokenizer), mix_trainded_model)\n",
        "sub_ds = alt_ds.filter(fil)\n",
        "sub_ds = sub_ds.map(preprocess_function, batch_size = batch_size*3, batched=True)\n",
        "sub_ds.set_format(type='torch', columns=columns_to_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5xUlS4dZW9s"
      },
      "source": [
        "## Training\n",
        "\n",
        "Requirements:  \n",
        "1. Provide Hyper parameters for finetuning process.\n",
        "2. If you want to push the model of HF hub, Make sure you have provided the correct `TOKEN` code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T17:12:36.887694Z",
          "iopub.status.busy": "2022-02-10T17:12:36.887421Z",
          "iopub.status.idle": "2022-02-10T17:12:37.016176Z",
          "shell.execute_reply": "2022-02-10T17:12:37.015483Z",
          "shell.execute_reply.started": "2022-02-10T17:12:36.887656Z"
        },
        "id": "qib4PArt2FjU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "source_lang = \"en\"\n",
        "target_lang = language_code\n",
        "model_name = f\"MarianMixFT_en-{language_code}\"\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    # Misc\n",
        "    output_dir = model_name, #_{source_lang}_to_{target_lang}\",\n",
        "    seed = 99,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    log_level = 'warning',\n",
        "    disable_tqdm = False,\n",
        "    \n",
        "    # Hyper parameters\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps = 10,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    remove_unused_columns = True,\n",
        "    fp16=True,\n",
        "\n",
        "    # Model backup\n",
        "    save_total_limit=3,\n",
        "    save_strategy = \"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=sub_ds[\"train\"],\n",
        "    eval_dataset=sub_ds[\"eval\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer= tokenizer,\n",
        "    compute_metrics= compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T17:12:37.017657Z",
          "iopub.status.busy": "2022-02-10T17:12:37.017418Z",
          "iopub.status.idle": "2022-02-10T18:21:03.784916Z",
          "shell.execute_reply": "2022-02-10T18:21:03.784288Z",
          "shell.execute_reply.started": "2022-02-10T17:12:37.017624Z"
        },
        "id": "3OFAtSJP2FjV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-10T18:21:03.791158Z",
          "iopub.status.busy": "2022-02-10T18:21:03.789076Z",
          "iopub.status.idle": "2022-02-10T18:23:11.147729Z",
          "shell.execute_reply": "2022-02-10T18:23:11.147059Z",
          "shell.execute_reply.started": "2022-02-10T18:21:03.79112Z"
        },
        "id": "md55j_kR2FjV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.predict(sub_ds[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GlojUWt2FjV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_name = f\"MarianMixFT_en-{language_code}\"\n",
        "\n",
        "model.push_to_hub(\n",
        "    model_name,\n",
        "    use_temp_dir= True,\n",
        "    use_auth_token=TOKEN\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Engine.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
